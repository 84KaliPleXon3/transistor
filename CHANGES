========================
  transistor: CHANGES
========================

11/24/18
- added a few attributes to SplashBrowser in anticipation of needing them
to support a more robust crawling class.

- SplashScraper now accepts a kwarg 'js_source' which forwards javascript to
the Lua script where it is executed with splash:runjs(splash.args.js_source).

11/20/2018
Made several API breaking changes:
- changed transistor.persistence.extractor.py ScrapedDataExtractor to
transistor.persistence.exporter.py BaseItemExporter and eliminated
the `.extract` method on that class as it is no longer needed.
- changed transistor.persistence.container SplashScraperContainer to
transistor.persistence.containers SplashScraperItems
- SplashScraperItems now inherits from class::Item() and defines Fields()
just like the Scrapy API. All the previous class attributes and properties
should now be accessed like a dictionary. For example, here is the old way:
    >>> result = get_job_results('books_scrape')
    >>> for r in result:
    >>>    book_titles.append(r.book_title)
Now, the new class::Item() API requires accessing `book_title` like below:
    >>>    book_titles.append(r['book_title'])
- changed transistor.workers.baseworker.py Baseworker.get_scraper_extractor()
method name to Baseworker.get_scraper_exporter()
- implemented built-in exporters closely matching Scrapy exporter API.
Exporters include 'BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',
'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter',
'JsonItemExporter', 'MarshalItemExporter'
- it is now possible to use custom serializers on a per field basis. Check
the examples/books_to_scrape/persistence/serialization.py file for an example.

11/19/2018
- implemented class::Item(), enabling a similar API to Scrapy for
Item data containers and Item Exporters.

11/18/2018
- added 'splash_wait' attribute to SplashScraper. This controls the time
in seconds Splash will wait after opening a web page, before taking actions.
This is particularly important for websites which may continue to load from
ajax after a time delay. Set with `splash_args=<float>` keyword. Default 3.0.
- changed API away from using the word `shell` to now using `container`.
- created newt_crud.py file in transistor/persistence/newt_db, removed
same from examples/books_to_scrape/persistence.
- moved container.py and extractor.py up a level into the main
transistor/persistence module


Nov 17, 2018
- pypy 0.1.1 release

Nov 16, 2018
- standardized SplashScraper attributes: 'auth', 'baseurl', 'browser', 'cookies',
'crawlera_user', 'http_session_timeout', 'http_session_valid', 'LUA_SOURCE',
'max_retries', 'name', 'number', 'referrer', 'searchurl', 'splash_args', 'user_agent'.

- Now, nearly all of the SplashScraper attributes can be set via **kwargs if desired.

Nov 15, 2018
-  when initializing a StatefulBook instance, use a **kwarg called `keywords` to set
the name of the spreadsheet column heading which contains the target search terms.
For example, keywords='titles' or keywords='part_numbers'. Defaults to "item".

Nov 13, 2018:
- Initial commit of public repo
- pypi 0.1.0 release.